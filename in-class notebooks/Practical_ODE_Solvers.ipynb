{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practical ODE solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error estimation\n",
    "We'd like to control the error in the solution:\n",
    "\n",
    "$$E = |U^n - u(t_n)|.$$\n",
    "\n",
    "Controlling the **global error** is very difficult and requires solving the problem over the whole time interval multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instead, what's usually done is to control the **local (one-step) error** ${\\mathcal L}^n$.  Since we don't have access to the exact solution, we can use two numerical solutions to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At step $n$, suppose we start with $U^n$ and compute two new solutions $U^{n+1}, \\hat{U}^{n+1}$, with methods of order $p$ and $p-1$ respectively.  Then the one-step errors for these solutions are\n",
    "\n",
    "\\begin{align*}\n",
    "    {\\mathcal L}^{n+1} = U^{n+1}-u(t_{n+1}) \\approx C_1 k^{p+1} \\\\\n",
    "    \\hat{\\mathcal L}^{n+1} = \\hat{U}^{n+1}-u(t_{n+1}) \\approx C_2 k^{p}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So if we take their difference, we get\n",
    "\n",
    "$$\n",
    "\\|{\\mathcal L}^{n+1} - \\hat{{\\mathcal L}}^{n+1}\\| = \\|U^{n+1}-u(t_{n+1}) - (\\hat{U}^{n+1}-u(t_{n+1})\\| \\approx\n",
    "  \\|C_1k^{p+1} - C_2 k^{p}\\| \\approx \\|C_2 k^p\\|\n",
    "$$\n",
    "\n",
    "as long as $k$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus the **difference between the two solutions** gives an approximation of the **error in the lower-order method**.  So we could continue stepping forward using the low-order solution $\\hat{U}^{n+1}$ and we'd have an approximation of the local error at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, it makes more sense to continue forward with the **high order solution**, for which the error is presumably even smaller.  This is called **local extrapolation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step size control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's say we want the one-step error at each step to be smaller than $\\epsilon$:\n",
    "\n",
    "$${\\mathcal L}^{n+1} < \\epsilon$$\n",
    "\n",
    "We can control the error by adjusting the step size $k$.  In fact we want to choose a step size that yields an error close to $\\epsilon$, since using a smaller timestep would be wasteful (doing \"too much work\" to get a needlessly accurate solution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus we want to choose a step size $k_*$ such that\n",
    "\n",
    "$$C_2 k_*^p \\approx \\epsilon.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose our computed error estimate value is $\\hat{\\epsilon}\\approx C_2 k^p$.  Thus we have\n",
    "\n",
    "$$\n",
    "\\left(\\frac{k_*}{k}\\right)^{p} \\approx \\frac{\\epsilon}{\\hat{\\epsilon}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "which means\n",
    "\n",
    "$$\n",
    "k_* = k \\left(\\frac{\\epsilon}{\\hat{\\epsilon}}\\right)^{1/p},\n",
    "$$\n",
    "\n",
    "so we should modify the timestep by a factor $\\left(\\frac{\\epsilon}{\\hat{\\epsilon}}\\right)^{1/p}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A full step size control algorithm looks like this:\n",
    "\n",
    " - Take a step and compute the approximate error $\\hat{\\epsilon}$\n",
    " - If $\\hat{\\epsilon}>\\epsilon$, reject the new solution value and go back to $t_n$.\n",
    " - If $\\hat{\\epsilon}\\le \\epsilon$, accept the new solution value and advance to $t_{n+1}$.\n",
    " - Either way, set the new step size to $k_* = \\alpha k \\left(\\frac{\\epsilon}{\\hat{\\epsilon}}\\right)^{1/p}$\n",
    " \n",
    "Here $\\alpha$ is a safety factor that is a little smaller than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More complicated approaches exist, based on optimal control theory and filtering methods from signal processing.  We won't go into the details of those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Embedded Runge-Kutta pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Getting an error estimate is nice, but having to compute two numerical solutions is expensive.  With Runge-Kutta methods, we can usually use the information we already have to get a second numerical approximation for free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that a Runge-Kutta method takes the form\n",
    "\n",
    "\\begin{align*}\n",
    "Y_i & = U^n + k\\sum_{j=1}^r a_{ij} f(Y_j,t_n+kc_j) & i=1,2,\\dots,r \\\\\n",
    "U^{n+1} & = U^n + k \\sum_{j=1}^r b_j f(Y_j, t_n+kc_j).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we use a second method of the form\n",
    "\n",
    "\\begin{align*}\n",
    "Y_i & = U^n + k\\sum_{j=1}^r a_{ij} f(Y_j,t_n+kc_j) & i=1,2,\\dots,r \\\\\n",
    "U^{n+1} & = U^n + k \\sum_{j=1}^r \\hat{b}_j f(Y_j, t_n+kc_j)\n",
    "\\end{align*}\n",
    "\n",
    "where the weights $a_{ij}$ for both methods are **the same** and only the weights $\\hat{b}_j$ are different, then the second method can be computed essentially **for free**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We call this an **embedded Runge-Kutta pair**.  We have to choose the second set of weights $\\hat{b}_j$ so that the second method is of a lower order and gives a good error estimate.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bogacki-Shampine RK3(2)4\n",
      "\n",
      " 0   |\n",
      " 1/2 | 1/2\n",
      " 3/4 |      3/4\n",
      " 1   | 2/9  1/3  4/9\n",
      "_____|____________________\n",
      "     | 2/9  1/3  4/9  0\n",
      "     | 7/24 1/4  1/3  1/8\n"
     ]
    }
   ],
   "source": [
    "from nodepy import rk\n",
    "rkm = rk.loadRKM('BS3')\n",
    "print(rkm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bogacki-Shampine RK3(2)4\n",
      "\n",
      " 0   |\n",
      " 1/2 | 1/2\n",
      " 3/4 |      3/4\n",
      " 1   | 2/9  1/3  4/9\n",
      "_____|____________________\n",
      "     | 2/9  1/3  4/9  0\n",
      "     | 7/24 1/4  1/3  1/8\n"
     ]
    }
   ],
   "source": [
    "print(rkm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This method has another useful property.  Notice that the last row of $A$ is equal to the weights $b$ of one method.  That means that stage $Y_3$ is also the new solution $U^{n+1}$.  When we take the next step, we'll need to evaluate $f(U^{n+1})$ anyway, so we can use $f(Y_3)$ \"for free\".\n",
    "\n",
    "In other words, even though this looks like a 4-stage method, we only need to do 3 derivative evaluations per step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we use a high-order-accurate method and the solution is smooth, we can take **very big steps**.  But sometimes we need output from the simulation at shorter intervals.\n",
    "\n",
    "It would be wasteful to use a small timestep just so that we can generate output at closer-spaced times.  Instead, we can use a cheaper method to generate highly accurate values **in between** the time step points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For linear multistep methods, because we need to store the last several steps anyway, we can simply use a high-order interpolating polynomial.  This polynomial can be evaluated at any point where we need output, and (since the step values are all high order), the result will be high order accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For Runge-Kutta methods, normally we only need to keep the most recent step in memory.  If we wanted to interpolate using just the step values $U^n, U^{n-1}, \\dots$ then we would need to keep several past steps in memory, which is costly and inconvenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instead, we could try to interpolate using the **stage values**.  For instance, using the fourth order method\n",
    "\n",
    "\\begin{align*}\n",
    "Y_1 & = U^n \\\\\n",
    "Y_2 & = U^n +\\frac{k}{2}f(t_n,Y_1) \\\\\n",
    "Y_3 & = U^n +\\frac{k}{2}f(t_n+k/2,Y_2) \\\\\n",
    "Y_4 & = U^n +kf(t_n+k/2,Y_3) \\\\\n",
    "U^{n+1} & = U^n + \\frac{k}{6} \\left(f(t_n,Y_1) + 2f(t_n+k/2,Y_2) + 2f(t_n+k/2,Y_3) + f(t_n+k,Y_4) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "we have approximate values at the start, middle, and end of the step.  Can we use these to get high-order output in between?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The simple answer is **no**, at least if we simply interpolate.  That's because the stage values at the middle point are **less accurate**.  For instance, $Y_2$ is based on an Euler step and is only a first-order approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous Runge-Kutta methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instead, we can design special interpolation formulas that take into account the structure and accuracy of the Runge-Kutta stages.  Our Runge-Kutta method reads\n",
    "\n",
    "\\begin{align*}\n",
    "Y_i & = U^n + k\\sum_{j=1}^r a_{ij} f(Y_j,t_n+kc_j) & i=1,2,\\dots,r \\\\\n",
    "U^{n+1} & = U^n + k \\sum_{j=1}^r b_j f(Y_j, t_n+kc_j).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The idea is to create an interpolant of the form\n",
    "\n",
    "$$\n",
    "U^{n+\\theta} \\approx u(t_n + \\theta k) \\approx U^n + k \\sum_{j=1}^r b_j(\\theta) f(Y_j),\n",
    "$$\n",
    "\n",
    "where $0\\le \\theta \\le 1$.  By carefully choosing the weight functions $b(\\theta)$, we can get high order\n",
    "accuracy in this way, without any extra storage or any extra derivative evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's an example of a 3-stage, 3rd-order method with a 2nd-order interpolation formula:\n",
    "\n",
    "\\begin{align*}\n",
    "    Y_1 & = U^n \\\\\n",
    "    Y_2 & = U^n + kf(Y_1) \\\\\n",
    "    Y_3 & = U^n + \\frac{k}{4}f(Y_1) + \\frac{k}{4}f(Y_2) \\\\\n",
    "    U^{n+1} & = U^n + k(f(Y_1) + f(Y_2) + 4 f(Y_3))/6 \\\\\n",
    "    U^{n+\\theta} & = U^n + (2\\theta-\\theta^2)f(Y_1) + (\\theta^2-2\\theta)f(Y_2) + \\theta f(Y_3)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For a method of order $p$, it is usually acceptable to use an interpolant of order $p-1$.  This is because the global error is one order worse than the local error and we won't use the interpolated values to continue stepping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic adaptation of the order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Should we use a high-order or low-order method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It depends on the accuracy we require and the smoothness of our solution.  Instead of trying to figure this out for each problem, we can design an algorithm that tries to figure it out for us, and chooses an appropriate high or low order method automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We could even change the order of the method being used during the computation, depending on the local smoothness of the solution.  There are several available codes that do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ODE solver libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nowadays many well-designed solvers with error estimation, step size control, dense output, and other features are widely available through packages in MATLAB, Python, Mathematica, and other languages.  Many of these are based on Fortran or C codes written in the late 20th century."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In Python, the most standard library for solving ODEs is `scipy.integrate.solve_ivp`.  Let's look at the solvers included there.  You can learn more about them by reading the help page:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html\n",
    "\n",
    "and the papers cited there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explicit SciPy solvers\n",
    "\n",
    "- `RK45`: A 7-stage embedded pair of orders 5 and 4, developed by Prince & Dormand.  Used with local extrapolation and a 4th-order continuous interpolant.  Because of the FSAL property, the cost is like that of a 6-stage method.\n",
    "- `RK23`: An embedded pair of orders 3 and 2, developed by Bogacki & Shampine.  Used with local extrapolation and 3rd-order continuous interpolant based on Hermite interpolation.\n",
    "- `DOP853`: An 8th-order method of Dormand & Prince with 12 stages, combined with two embedded error estimators of order 5 and 3, and a dense output interpolant.\n",
    "\n",
    "Higher-order methods cost more per step (since they use more stages) but allow larger steps as long as the solution is smooth.  For very small error tolerances, DOP853 will be most efficient, while at very loose tolerances RK23 will be most efficient.  RK45 is recommended as a good method to try first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implicit SciPy solvers\n",
    "- `BDF`: Uses \"Numerical differentiation methods\" of order 1-5.  These are a small modification of the BDF methods, to get better stability regions.  Automatically adapts the order.  Uses a Newton method to solve for each step.  Reuses the Jacobian from a previous step until this causes slow convergence. \n",
    "- `Radau`: A 5th-order fully-implicit Runge-Kutta method with a 3rd-order embedded error estimator and 3rd-order interpolant.\n",
    "- `LSODA`: This one tries to figure out for you if the problem is stiff, and choose an appropriate method.  Also adapts the order and step size automatically.  Uses Adams-Bashforth methods of order 1-12 for nonstiff problems and BDF methods of order 1-5 for stiff problems.  Due to Petzold & Hindmarsh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explicit MATLAB solvers\n",
    "- `ode23`: Same as SciPy's `RK23`\n",
    "- `ode45`: Same as SciPy's `RK45`\n",
    "- `ode113`: A group of linear multistep methods of order 1-13.  Starts with order 1 method (forward Euler) and then increases the order gradually based on measurement of the error.  Uses a predictor-corrector approach, combining Adams-Bashforth and Adams-Moulton methods.  It can be the best option of the problem is *mildly stiff* and evaluating $f$ is very expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implicit MATLAB solvers\n",
    "- `ode15s`: like SciPy's `BDF`\n",
    "- `ode23s`: Rosenbrock method.  Like implicit Runge-Kutta, but only performs one Newton iteration (instead of iterating to convergence).  2nd-order method is L-stable; 3rd-order method is A-stable and used for error estimation.  Has a 2nd-order interpolant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
